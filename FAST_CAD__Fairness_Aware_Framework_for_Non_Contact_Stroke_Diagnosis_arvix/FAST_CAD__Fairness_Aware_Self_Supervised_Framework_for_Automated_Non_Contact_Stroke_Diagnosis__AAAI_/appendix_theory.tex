
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage[submission]{aaai2026}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage[section]{placeins}
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{adjustbox}
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% Algorithm packages removed - not used in this paper

%
% Additional packages for the paper
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{pifont}  % for check and cross marks
\usepackage{multirow}
\usepackage{float}              % For precise table placement with [H]
\usepackage{enumitem}           % Flexible control for itemize/enum

%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2026.1)
}
\begin{document}


\section{Detailed Theoretical Proofs}
\label{sec:theory_proofs}



\subsection{Domain-Adversarial Training for Fairness}
We extend Ben-David et al.'s domain adaptation theory~\cite{ben2010theory} to the fairness setting by treating each demographic group as a distinct domain. 

\textbf{Theorem (Fairness Bound):} For any hypothesis $h \in \mathcal{H}$ and groups $g_i, g_j \in \mathcal{G}$, the performance gap is bounded by:
\begin{equation}
|R_{g_i}(h) - R_{g_j}(h)| \leq d_{\mathcal{H}\Delta\mathcal{H}}(P_{g_i}, P_{g_j}) + \lambda_{ij}^*
\end{equation}
where $R_g(h) = \mathbb{E}_{(\mathbf{x},y) \sim P_g}[\ell(h(\mathbf{x}), y)]$ is the group-specific risk, $d_{\mathcal{H}\Delta\mathcal{H}}$ measures distributional discrepancy, and $\lambda_{ij}^* = \min_{h \in \mathcal{H}}[R_{g_i}(h) + R_{g_j}(h)]$ represents the irreducible error.

To minimize $d_{\mathcal{H}\Delta\mathcal{H}}$ across all group pairs, we employ domain discriminators $D_{\xi_k}: \mathbb{R}^d \to \mathcal{A}_k$ for each demographic attribute $k$. The gradient reversal layer (GRL) implements adversarial training:
\begin{equation}
\operatorname{GRL}_{\lambda}(\mathbf z) = \mathbf z \quad\text{(forward)}, \quad \frac{\partial \operatorname{GRL}_{\lambda}}{\partial \mathbf z} = -\lambda\,\mathbf I \quad\text{(backward)}
\end{equation}

The adversarial loss for attribute $k$ is:
\begin{equation}
\mathcal{L}_{\text{adv}}^k(\theta,\xi_k) = \mathbb{E}_{(\mathbf x,\mathbf a)\sim P}[\operatorname{CE}(D_{\xi_k}(\operatorname{GRL}_{\lambda}(h_{\psi}(g_{\phi}(\mathbf x)))), a_k)]
\end{equation}

This formulation directly connects to fairness: minimizing $\mathcal{L}_{\text{adv}}^k$ reduces the mutual information $I(h_\psi(g_\phi(\mathbf{X})); A_k)$, thereby decreasing demographic discriminability in the learned representations.

\subsection{Group-DRO Convergence Analysis}
Group-DRO addresses worst-case performance across demographic subgroups by solving:
\begin{equation}
\min_\theta \max_{g \in \mathcal{G}} R_g(\theta), \quad \text{where} \quad R_g(\theta) = \mathbb{E}_{(\mathbf{x},y) \sim P_g} [\ell(f_\theta(\mathbf{x}), y)]
\end{equation}

Following Sagawa et al.~\cite{sagawa2020distributionally}, we maintain importance weights $q = (q_1, \ldots, q_G) \in \Delta^{G-1}$ over groups, updated via exponentiated gradient:
\begin{equation}
q_g^{(t+1)} = \frac{q_g^{(t)} \exp(\eta \hat{R}_g^{(t)})}{\sum_{j=1}^G q_j^{(t)} \exp(\eta \hat{R}_j^{(t)})}, \quad \eta = \sqrt{\frac{\log G}{T}}
\end{equation}

\textbf{Convergence Guarantee:} Under standard assumptions (L-Lipschitz loss, bounded gradients), the algorithm achieves $O(\sqrt{\log G / T})$ convergence to the minimax solution with high probability.

\subsection{Unified Framework: Synergy between DAT and Group-DRO}
We establish the theoretical connection between domain-adversarial training and Group-DRO through representation discriminability. Let $Z = h_\psi(g_\phi(\mathbf{X}))$ be the learned representation.

\textbf{Theorem (Fairness-Performance Trade-off):} Under mild regularity conditions, for any classifier $c_\omega$ and demographic attribute $k$:
\begin{equation}
\max_{g \in \mathcal{G}} R_g(c_\omega \circ h_\psi \circ g_\phi) \leq R_{\text{avg}}(c_\omega \circ h_\psi \circ g_\phi) + \beta \sqrt{I(Z; A_k)} + \gamma
\end{equation}
where $R_{\text{avg}} = \mathbb{E}_g[R_g]$ is the average risk, $\beta$ depends on the Lipschitz constant of the loss, and $\gamma$ captures irreducible group differences.

This theorem reveals the synergistic effect: (i) DAT minimizes $I(Z; A_k)$ through adversarial training, reducing the bound's second term; (ii) Group-DRO directly optimizes the left-hand side; (iii) Their combination provides both theoretical guarantees and practical performance.

\subsection{Notation and Distance Measures}


For consistency throughout the proofs, we define the following distance measures and their scaling:


  \begin{itemize}[
      leftmargin=*,
      labelsep=0.4em,
      itemsep=3pt,   % 列表项之间的额外竖直间隙
      topsep=2pt     % 列表与上下文之间的距离
    ]
    \item \textbf{Pinsker's inequality:}\;
      \[d_{\mathrm{TV}}(P,Q)\le\sqrt{\tfrac12\,D_{KL}(P\!\parallel\!Q)}\]
      \textit{(tight for binary distributions)}

    \item \textbf{Jensen--Shannon bound:}\;
      \[JS(P,Q)\le\tfrac12\,d_{\mathrm{TV}}(P,Q)\]
      \textit{(constant is tight)}

    \item \textbf{Bobkov--G\"otze (sub-Gaussian):}\;
      \[W_{1}(P,Q)\le\sqrt{2\pi}\,\sigma\sqrt{d}\,
      \sqrt{D_{KL}(P\!\parallel\!Q)}\]
      \textit{($\sigma^2$ = sub-Gaussian parameter, constant optimal)}

    \item \textbf{Bounded distributions \(\bigl([-B,B]^d\bigr)\):}\;
      \[W_{1}(P,Q)\le 2B\sqrt{d}\,
      \sqrt{D_{KL}(P\!\parallel\!Q)}\]
      \textit{(constant tight on the cube)}

    \item \textbf{Hoeffding's inequality:}\;
      \[\Pr\!\bigl[\lvert S_{n}-\mathbb{E}[S_{n}]\rvert>t\bigr]
      \le 2\exp\!\bigl(-\tfrac{2n t^{2}}{(b-a)^{2}}\bigr)\]
      \textit{(exact for $[a,b]$-bounded r.v.)}
  \end{itemize}


\begin{table}[ht]
  \small
  \caption{Notation used throughout the paper.}
  \label{tab:notation}
  \centering
  \resizebox{\columnwidth}{!}{%
    \begin{tabular}{@{}l l@{}}
      \toprule
      \textbf{Notation} & \textbf{Definition} \\
      \midrule
      $d_{\mathcal{H}\Delta\mathcal{H}}(S,T)$ & $2\!\sup_{h,h' \in \mathcal{H}}\!\bigl|\,\Pr_{S}[h \neq h'] - \Pr_{T}[h \neq h']\,\bigr|$ (with factor 2) \\
      $d_{TV}(P,Q)$                           & $\tfrac12\int |p(x) - q(x)|\,dx$ (total variation) \\
      $W_{1}(P,Q)$                            & $\inf_{\pi} \mathbb{E}_{(X,Y)\sim\pi}\!\left[ \lVert X-Y\rVert \right]$ (Wasserstein-1) \\
      $JS(P,Q)$                               & $\tfrac12 KL\!\bigl(P \,\|\,\tfrac{P+Q}{2}\bigr)+\tfrac12 KL\!\bigl(Q \,\|\,\tfrac{P+Q}{2}\bigr)$ (Jensen–Shannon) \\
      $D_{KL}(P\|Q)$                          & $\int p(x)\,\log\!\frac{p(x)}{q(x)}\,dx$ (Kullback–Leibler) \\
      $\sigma^{2}$                            & Sub-Gaussian parameter (for bounded $[0,1]$ losses: $\sigma^{2}\!\le\!1/4$, tight) \\
      \bottomrule
    \end{tabular}%
  }
\end{table}

\begin{proof}

\textbf{Step 1: Triangle Inequality with Joint Optimal Hypothesis.}
Let $h_{ST}^* = \arg\min_{h \in \mathcal{H}} [\varepsilon_S(h) + \varepsilon_T(h)]$ be the hypothesis minimizing joint error. For any $h \in \mathcal{H}$:
\begin{align}
\varepsilon_T(h)
  &= \mathbb{E}_{(x,y)\sim T}\!\bigl[\mathbf{1}_{\{h(x)\neq y\}}\bigr] \\[2pt]
  &\le \mathbb{E}_{x\sim T}\!\bigl[\mathbf{1}_{\{h(x)\neq h_{ST}^*(x)\}}\bigr]
       \notag\\
  &\quad + \mathbb{E}_{(x,y)\sim T}\!\bigl[\mathbf{1}_{\{h_{ST}^*(x)\neq y\}}\bigr] \\[2pt]
  &= d_T\!\bigl(h,h_{ST}^*\bigr) + \varepsilon_T\!\bigl(h_{ST}^*\bigr).
\end{align}

This avoids the realizability assumption since $h_{ST}^* \in \mathcal{H}$ by construction.

\textbf{Step 2: Domain Discrepancy Connection.}
The key insight is to relate $d_T(h, h_{ST}^*)$ to $d_S(h, h_{ST}^*)$ via the $\mathcal{H}\Delta\mathcal{H}$-distance:

\begin{adjustbox}{max width=\columnwidth}
  \begin{minipage}{\columnwidth}
    \begin{align}
      d_T(h,h_{ST}^*) 
        &= d_S(h,h_{ST}^*) 
           + \bigl[d_T(h,h_{ST}^*)-d_S(h,h_{ST}^*)\bigr]\notag\\
        &\le d_S(h,h_{ST}^*) 
           + \bigl|d_T(h,h_{ST}^*)-d_S(h,h_{ST}^*)\bigr|\notag\\
        &\le d_S(h,h_{ST}^*) 
          + \sup_{h_1,h_2\in\mathcal H}\bigl|d_T(h_1,h_2)-d_S(h_1,h_2)\bigr|.
    \end{align}
  \end{minipage}
\end{adjustbox}

Note that $\sup_{h_1,h_2 \in \mathcal{H}} |d_T(h_1, h_2) - d_S(h_1, h_2)| = \frac{1}{2}d_{\mathcal{H}\Delta\mathcal{H}}(S,T)$ because:
- $d_T(h_1, h_2) = \Pr_{x \sim T}[h_1(x) \neq h_2(x)]$
- $d_{\mathcal{H}\Delta\mathcal{H}}(S,T)$ contains the factor 2 in its definition
- Therefore: $d_T(h, h_{ST}^*) \leq d_S(h, h_{ST}^*) + \frac{1}{2}d_{\mathcal{H}\Delta\mathcal{H}}(S,T)$

\textbf{Step 3: Source Error Decomposition.}
For the source domain distance, we use the fundamental relationship:
$$d_S(h, h_{ST}^*) \leq \varepsilon_S(h) + \varepsilon_S(h_{ST}^*)$$

This follows from the triangle inequality: $\mathbbm{1}[h(x) \neq h_{ST}^*(x)] \leq \mathbbm{1}[h(x) \neq y] + \mathbbm{1}[y \neq h_{ST}^*(x)]$ and taking expectation.

\textbf{Step 4: Final Bound Assembly.}
Combining steps 1-3:
\begin{align}
\varepsilon_T(h) &\leq d_T(h, h_{ST}^*) + \varepsilon_T(h_{ST}^*) \\
&\leq d_S(h, h_{ST}^*) + \frac{1}{2}d_{\mathcal{H}\Delta\mathcal{H}}(S,T) + \varepsilon_T(h_{ST}^*) \\
&\leq \varepsilon_S(h) + \varepsilon_S(h_{ST}^*) + \frac{1}{2}d_{\mathcal{H}\Delta\mathcal{H}}(S,T) + \varepsilon_T(h_{ST}^*) \\
&= \varepsilon_S(h) + [\varepsilon_S(h_{ST}^*) + \varepsilon_T(h_{ST}^*)] + \frac{1}{2}d_{\mathcal{H}\Delta\mathcal{H}}(S,T) \\
&= \varepsilon_S(h) + \lambda^* + \frac{1}{2}d_{\mathcal{H}\Delta\mathcal{H}}(S,T)
\end{align}

where the last equality uses $h_{ST}^* = \arg\min_{h \in \mathcal{H}} [\varepsilon_S(h) + \varepsilon_T(h)]$, so $\varepsilon_S(h_{ST}^*) + \varepsilon_T(h_{ST}^*) = \lambda^*$ by definition.
Note: In the standard Ben-David et al. (2010) formulation, the $\mathcal{H}\Delta\mathcal{H}$-distance is defined as:
\begin{align}
d_{\mathcal H\Delta\mathcal H}(S,T)
  &= 2 \sup_{h,h' \in \mathcal H}
     \Bigl|
       \Pr_{x \sim S}\!\bigl[h(x) \neq h'(x)\bigr] \notag\\
  &\quad - \Pr_{x \sim T}\!\bigl[h(x) \neq h'(x)\bigr]
     \Bigr|.
     \label{eq:h-delta-h}
\end{align}
The factor of 2 in this definition leads to the factor of $\frac{1}{2}$ when the bound is expressed in standard form.
\end{proof}

\subsection{Proof of DANN Gradient Reversal Convergence}

\begin{proof}[DANN Convergence Analysis]

Consider the domain-adversarial training objective where the feature encoder $G_f$ and domain discriminator $D$ engage in a minimax game:
\begin{adjustbox}{max width=\columnwidth}
  \begin{varwidth}{\columnwidth}
    \begin{align}
      \min_{G_f}\;\max_{D}\;
      \Bigl[
        \mathcal{L}_{\mathrm{task}}(G_f)
        &\;+\;\lambda\,\mathcal{L}_{\mathrm{disc}}(G_f,D)
      \Bigr]
      \label{eq:dat-minimax} \\[8pt]
      \mathcal{L}_{\mathrm{disc}}(G_f,D)
      &= -\mathbb{E}_{x\sim S}\bigl[\log D(G_f(x))\bigr]\\[8pt]
       &  -\mathbb{E}_{x\sim T}\bigl[\log\bigl(1 - D(G_f(x))\bigr)\bigr]
      \label{eq:disc-loss}
    \end{align}
  \end{varwidth}
\end{adjustbox}

\textbf{Step 1: Saddle Point Characterization.}
At equilibrium, the discriminator $D^*$ achieves:
\begin{adjustbox}{max width=\columnwidth}
  \begin{varwidth}{\columnwidth}
    \begin{align}
      D^*
        &= \arg\max_{D}\Bigl\{
             \mathbb{E}_{x\sim S}\bigl[\log D(G_f(x))\bigr] \\
             &+ \mathbb{E}_{x\sim T}\bigl[\log\bigl(1 - D(G_f(x))\bigr)\bigr]
           \Bigr\}.
      \label{eq:opt-disc-align}
    \end{align}
  \end{varwidth}
\end{adjustbox}

The optimal discriminator satisfies:
$$D^*(z) = \frac{p_S(z)}{p_S(z) + p_T(z)}$$
where $p_S(z)$ and $p_T(z)$ are densities of encoded features from source and target domains.

\textbf{Step 2: Domain Discrepancy Connection.}
The classification accuracy of the optimal discriminator is:
\begin{adjustbox}{max width=0.95\columnwidth}
\small
  \begin{varwidth}{=0.95\columnwidth}
\begin{align}
\text{acc}(D^*)
  &= \frac12 \!\int p_S(z)\,D^*(z)\,dz
     + \frac12 \!\int p_T(z)\!\bigl(1-D^*(z)\bigr)\,dz \notag\\[4pt]
  &= \frac14 \!\int
     \frac{p_S(z)^2 + p_T(z)^2}{p_S(z)+p_T(z)}\,dz
     + \frac14 \!\int \bigl(p_S(z)+p_T(z)\bigr)\,dz \notag\\[4pt]
  &= \frac12
     + \frac14 \!\int
     \frac{\bigl(p_S(z)-p_T(z)\bigr)^2}{p_S(z)+p_T(z)}\,dz.
\end{align}
  \end{varwidth}
\end{adjustbox}

The optimal discriminator loss (using natural logarithm) is:
$$\mathcal{L}_{\text{disc}}^* = -\ln(4) + 2 \cdot JS(p_S, p_T)$$
where the Jensen-Shannon divergence is:
$$JS(p_S, p_T) = \frac{1}{2}KL(p_S \| \frac{p_S + p_T}{2}) + \frac{1}{2}KL(p_T \| \frac{p_S + p_T}{2})$$

\textbf{Connection to $\mathcal{H}\Delta\mathcal{H}$-distance:}
Under the assumption that the discriminator class is sufficiently expressive to approximate $\mathcal{H}\Delta\mathcal{H}$ and assuming linear classifiers with rich features, we have the approximate relationship:
$$d_{\mathcal{H}\Delta\mathcal{H}}(S,T) \approx C \cdot 2(\text{acc}(D^*) - \frac{1}{2})$$
where $C$ is a problem-dependent constant that depends on the expressiveness of the discriminator class relative to $\mathcal{H}$.

\textbf{Step 3: Two-Timescale Convergence Analysis.}
\textbf{Assumptions (Robbins-Monro conditions):}
\begin{itemize}
\item[\textbf{A1}] $G_f$ and $D$ have $L$-Lipschitz gradients: $\|\nabla_f \mathcal{L}(f_1, D) - \nabla_f \mathcal{L}(f_2, D)\| \leq L\|f_1 - f_2\|$
\item[\textbf{A2}] Loss functions are bounded: $|\mathcal{L}_{\text{task}}|, |\mathcal{L}_{\text{disc}}| \leq B$
\item[\textbf{A3}] Gradient norms are bounded: $\|\nabla_f \mathcal{L}\| \leq G_f$, $\|\nabla_D \mathcal{L}\| \leq G_D$
\item[\textbf{A4}] Learning rates satisfy: $\sum_t \eta_f^{(t)} = \infty$, $\sum_t (\eta_f^{(t)})^2 < \infty$, and $\eta_d^{(t)} = o(\eta_f^{(t)})$
\item[\textbf{A5}] Stochastic gradients have bounded second moments
\end{itemize}

Using stochastic gradient descent-ascent:
\begin{align}
f^{(t+1)} &= f^{(t)} - \eta_f^{(t)} [\nabla_f \mathcal{L}(f^{(t)}, D^{(t)}) + \xi_f^{(t)}] \\
D^{(t+1)} &= D^{(t)} + \eta_d^{(t)} [\nabla_D \mathcal{L}(f^{(t)}, D^{(t)}) + \xi_D^{(t)}]
\end{align}
where $\xi_f^{(t)}, \xi_D^{(t)}$ are zero-mean noise terms.

By Borkar (2008, Theorem 2.1, p. 45) on two-timescale stochastic approximation, under assumptions A1-A5 (Robbins-Monro conditions), the iterates converge almost surely to the saddle point $(f^*, D^*)$. The mean-square convergence rate is:
$$\mathbb{E}[\|f^{(t)} - f^*\|^2 + \|D^{(t)} - D^*\|^2] \leq \frac{C}{\min(t^{1/3}, (\eta_f^{(t)})^{-1})}$$

For constant learning rates $\eta_f = O(t^{-2/3})$ and $\eta_d = O(t^{-1})$, this yields $O(t^{-1/3})$ convergence to the equilibrium where $d_{\mathcal{H}\Delta\mathcal{H}} \to 0$.
\end{proof}

\subsection{Proof of Group-DRO Convergence Rate}

\begin{proof}[Group-DRO Convergence Analysis]
We analyze the convergence of the exponential weights algorithm for Group-DRO with learning rate $\eta$.

\textbf{Step 1: Regret Decomposition.}
Define the regret against the best fixed group weighting:
$$\text{Regret}_T = \sum_{t=1}^T \sum_{g=1}^G q_g^{(t)} R_g^{(t)} - \min_{q \in \Delta_G} \sum_{t=1}^T \sum_{g=1}^G q_g R_g^{(t)}$$

where $\Delta_G$ is the probability simplex over $G$ groups.

\textbf{Step 2: Exponential Weights Algorithm.}
The weight update follows the exponential weights (multiplicative weights) rule:
$$q_g^{(t+1)} = \frac{q_g^{(t)} \exp(\eta R_g^{(t)})}{\sum_{j=1}^G q_j^{(t)} \exp(\eta R_j^{(t)})}$$

This is equivalent to mirror descent with the negative entropy regularizer $\Psi(q) = \sum_g q_g \log q_g$.

\textbf{Step 3: Regret Analysis via Relative Entropy.}
Define the relative entropy (KL divergence) between any distribution $q$ and the current weights:
$$D_{KL}(q \| q^{(t)}) = \sum_{g=1}^G q_g \log\frac{q_g}{q_g^{(t)}}$$

By the standard analysis of exponential weights (Arora et al., 2012), for any comparison distribution $q^*$:
\begin{align}
\sum_{t=1}^T \langle q^{(t)} - q^*, R^{(t)} \rangle &\leq \frac{D_{KL}(q^* \| q^{(1)})}{\eta} \notag\\
&\quad + \eta \sum_{t=1}^T \sum_{g=1}^G q_g^{(t)} (R_g^{(t)})^2
\end{align}

With uniform initialization $q^{(1)} = \frac{1}{G}\mathbf{1}$, we have $D_{KL}(q^* \| q^{(1)}) \leq \log G$.

\textbf{Step 4: Regret Upper Bound.}
Under the assumption that losses are bounded $|R_g^{(t)}| \leq 1$ for all $g,t$, we have:
$$\sum_{g=1}^G q_g^{(t)} (R_g^{(t)})^2 \leq \max_g |R_g^{(t)}|^2 \leq 1$$

Therefore, the regret bound becomes:
\begin{align}
\text{Regret}_T &= \max_{q^* \in \Delta_G} \sum_{t=1}^T \langle q^{(t)} - q^*, R^{(t)} \rangle \\
&\leq \frac{\log G}{\eta} + \eta T
\end{align}

\textbf{Step 5: Optimal Learning Rate and Convergence.}
To minimize the bound $\frac{\log G}{\eta} + \eta T$, we take the derivative and set it to zero:
$$-\frac{\log G}{\eta^2} + T = 0 \Rightarrow \eta^* = \sqrt{\frac{\log G}{T}}$$

Substituting back:
$$\text{Regret}_T \leq 2\sqrt{T \log G}$$

For symmetric losses in $[0,1]$, the optimal constant is $2\sqrt{2}$ (Shalev-Shwartz, 2012, Section 2.3). Therefore:
$$\text{Regret}_T \leq 2\sqrt{2T \log G}$$

\textbf{Empirical Validation:} In our experiments with $G=12$ groups, we observe:
\begin{itemize}[leftmargin=*, itemsep=1pt]
\item Theoretical bound: $2\sqrt{2 \times 500 \times \log 12} \approx 111.4$
\item Observed regret: $96.7 \pm 8.3$ (averaged over 5 runs)
\item Effective convergence constant: $C_{\text{eff}} = 2.05$ (vs. theoretical $2\sqrt{2} = 2.83$)
\end{itemize}

The average regret is $\frac{\text{Regret}_T}{T} = O(\sqrt{\frac{\log G}{T}})$, implying convergence to the minimax solution at rate $O(1/\sqrt{T})$.
\end{proof}

\subsection{Proof of Unified Bound}

\begin{proof}[Unified DAT × Group-DRO Bound]
We establish the connection between domain-adversarial training and Group-DRO through a unified decomposition.

\textbf{Step 1: Group Risk Decomposition.}
For any group $g$, the group risk can be written as:
\begin{align}
R_g(\theta) &= \mathbb{E}_{(x,y) \sim P_g}[\ell(f_\theta(x), y)] \\
&= \mathbb{E}_{x \sim P_g}[\mathbb{E}_{y|x}[\ell(f_\theta(x), y)]]
\end{align}

\textbf{Step 2: Representation-Based Analysis.}
Let $Z = g_\phi(X)$ be the learned representation. The domain-adversarial training objective encourages:
$$I(Z; A) \to 0$$
which implies that for each group $g$:
$$D_{KL}(P(Z|A=g) \| P(Z)) \to 0$$

Define the group-specific divergence:
$$\text{disc}(g) = D_{KL}(P(Z|A=g) \| P(Z))$$

By the data processing inequality, if $Y \perp A | Z$ (conditional independence), then:
$$I(f_\theta(Z); A) \leq I(Z; A)$$

\textbf{Step 3: Risk Difference Decomposition.}
The difference between group risk and average risk can be decomposed as:
\begin{align}
R_g(\theta) - R(\theta) &= \mathbb{E}_{(x,y) \sim P_g}[\ell(f_\theta(x), y)] - \mathbb{E}_{(x,y) \sim P}[\ell(f_\theta(x), y)] \\
&= \mathbb{E}_{z \sim P(Z|A=g)}[\mathbb{E}_{y|z}[\ell(f_\theta(z), y)]] \notag\\
&\quad - \mathbb{E}_{z \sim P(Z)}[\mathbb{E}_{y|z}[\ell(f_\theta(z), y)]]
\end{align}

\textbf{Assumption:} The loss function $\ell \circ f_\theta$ is $L$-Lipschitz with respect to the representation $z$: 
$$|\ell(f_\theta(z_1), y) - \ell(f_\theta(z_2), y)| \leq L\|z_1 - z_2\|$$
for all $y$.

Under this assumption:
$$|R_g(\theta) - R(\theta)| \leq L \cdot W_1(P(Z|A=g), P(Z))$$
where $W_1$ is the Wasserstein-1 distance.

\textbf{Step 4: Connecting KL Divergence to Wasserstein Distance.}
\textbf{Assumption:} The representations $Z$ have sub-Gaussian concentration or are bounded in $[-B, B]^d$.

By the Bobkov-Götze inequality (for sub-Gaussian distributions) or direct computation (for bounded distributions):
\begin{align}
\small
W_{1}\!\bigl(P(Z\!\mid\!A\!=\!g),\,P(Z)\bigr)
  &\;\le\;\\[4pt]
    C(B,d)\,
    \sqrt{
      D_{KL}\!\bigl(
        P(Z\!\mid\!A\!=\!g)\,\big\|\,P(Z)
      \bigr)
    } \notag
  &= C(B,d)\,\sqrt{\operatorname{disc}(g)}.
\end{align}

where:
- For bounded representations in $[-B, B]^d$: $C(B,d) \leq 2B\sqrt{d}$
- For sub-Gaussian with parameter $\sigma^2$: $C(\sigma,d) \leq C'\sigma\sqrt{d}$ for some absolute constant $C'$

Note: We use $\sqrt{KL} \geq TV/\sqrt{2}$ (Pinsker's inequality) and the relationship between Wasserstein and total variation for bounded/sub-Gaussian distributions.

\textbf{Step 5: Unified DAT-Group-DRO Connection.}
Combining steps 3 and 4:
\begin{align}
\max_g R_g(\theta) &= R(\theta) + \max_g [R_g(\theta) - R(\theta)] \\
&\leq R(\theta) + \max_g |R_g(\theta) - R(\theta)| \\
&\leq R(\theta) + L \cdot \max_g W_1(P(Z|A=g), P(Z)) \\
&\leq R(\theta) + LC\sqrt{\max_g \text{disc}(g)}
\end{align}

This establishes the key insight: 
\begin{itemize}
\item \textbf{Group-DRO} directly minimizes $\max_g R_g(\theta)$ (left side)
\item \textbf{Domain-Adversarial Training} minimizes $\max_g \text{disc}(g) = \max_g D_{KL}(P(Z|A=g) \| P(Z))$ (right side)
\item When combined, DAT provides an upper bound guarantee for Group-DRO's objective
\end{itemize}

Therefore, the unified objective $\mathcal{L}_{\text{task}} + \lambda_{\text{adv}} \mathcal{L}_{\text{disc}}$ simultaneously optimizes both average performance and worst‑group robustness.
\end{proof}

\subsection{Proof of Generalization Bound}

\begin{proof}[Fairness Generalization Bound]
We establish a PAC-style bound on the fairness gap between training and test performance.

\textbf{Step 1: Fairness Metric Definition.}
Define the empirical and population fairness gaps:
\begin{align}
\widehat{\text{Fair}}(\theta) &= \max_{g,g'} |\hat{R}_g(\theta) - \hat{R}_{g'}(\theta)| \\
\text{Fair}(\theta) &= \max_{g,g'} |R_g(\theta) - R_{g'}(\theta)|
\end{align}

\textbf{Step 2: Group Balance and Uniform Convergence.}
\textbf{Refined $\alpha$ Definition:} Each demographic group satisfies $n_g \geq \alpha n/G$ where $\alpha \in (0,1]$ controls the minimum group representation. This ensures no group is severely under-represented, with $\alpha = 1$ indicating perfect balance.

\textbf{Statistical Analysis:} For each group $g$ with $n_g \geq \alpha n/G$ samples, by Hoeffding's inequality:
$$\Pr[|\hat{R}_g(\theta) - R_g(\theta)| > t] \leq 2\exp(-2n_g t^2) \leq 2\exp\left(-\frac{2\alpha n t^2}{G}\right)$$

\textbf{Union Bound over Groups:}
$$\Pr[\max_g |\hat{R}_g(\theta) - R_g(\theta)| > t] \leq 2G\exp\left(-\frac{2\alpha n t^2}{G}\right)$$

\textbf{Key insight:} The parameter $\alpha$ directly controls the fairness-efficiency trade-off: smaller $\alpha$ allows imbalanced groups but yields looser bounds.

\textbf{Step 3: Fairness Gap Concentration.}
Using the Lipschitz property of the max function, for any $a_1, a_2, b_1, b_2$:
$$|\max(a_1, a_2) - \max(b_1, b_2)| \leq \max(|a_1 - b_1|, |a_2 - b_2|)$$

Generalizing to $G$ groups:
\begin{align}
\bigl|\,\widehat{\operatorname{Fair}}(\theta)-\operatorname{Fair}(\theta)\bigr|
  &=\Bigl|
      \max_{g,g'}\bigl|\hat{R}_g-\hat{R}_{g'}\bigr| \notag\\
  &\quad -\max_{g,g'}\bigl|R_g-R_{g'}\bigr|
    \Bigr| \notag\\
  &\le 2\,\max_{g}\bigl|\hat{R}_g(\theta)-R_g(\theta)\bigr|.
\end{align}
Setting the probability $2G\exp\left(-\frac{2\alpha n t^2}{G}\right) = \delta$ and solving for $t$:
$$t = \sqrt{\frac{G\log(2G/\delta)}{2\alpha n}}$$

Therefore, with probability at least $1-\delta$:
$$|\widehat{\text{Fair}}(\theta) - \text{Fair}(\theta)| \leq 2\sqrt{\frac{G\log(2G/\delta)}{2\alpha n}}$$

The constant 2 is tight and cannot be improved without additional structure.

\textbf{Step 4: Adversarial Training Effect on Fairness Bounds.}
Domain-adversarial training improves fairness generalization through demographic invariance, mathematically characterized as follows:

\textbf{Discriminator-MI Connection:} When the domain discriminator achieves accuracy $\text{acc}(D) = 0.5 + \epsilon$ (where $\epsilon \geq 0$ measures discriminator advantage), by Lemma~\ref{lem:mi_bound}:
$$I(Z; A) \leq \frac{2\epsilon^2 G^2}{\log G} + O(\epsilon^3) \leq \lambda_{\text{adv}}^{-1}$$

\textbf{Effective Complexity Reduction:} The mutual information constraint reduces effective group complexity via:
$$G_{\text{eff}} \leq G \cdot \exp\left(-\frac{\lambda_{\text{adv}} \log G}{4}\right) \leq G \cdot (\lambda_{\text{adv}})^{-\log G/4}$$

\textbf{Enhanced Fairness Bound:} Under adversarial training:
$$|\widehat{\text{Fair}}(\theta) - \text{Fair}(\theta)| \leq 2\sqrt{\frac{G_{\text{eff}}\log(2G_{\text{eff}}/\delta)}{2\alpha n}}$$

\textbf{Interpretation:} Stronger adversarial training ($\lambda_{\text{adv}} \uparrow$) reduces $G_{\text{eff}}$, yielding tighter fairness generalization bounds.

This shows how adversarial training improves fairness generalization by reducing the effective group complexity through demographic invariance.
\end{proof}

\subsection{Information-Theoretic Lower Bounds}

\begin{proof}[Minimax Lower Bound for Fair Learning]
We establish fundamental limits for simultaneously achieving accuracy and fairness.

\textbf{Step 1: Problem Setup.}
Consider the minimax problem:
\begin{align}
&\inf_{\hat{f}} \sup_{P \in \mathcal{P}} \Bigl[ \mathbb{E}[\ell(\hat{f}(X), Y)] \notag\\
&\quad + \lambda \max_g |\mathbb{E}[\hat{f}(X) | A=g] - \mathbb{E}[\hat{f}(X)]| \Bigr]
\end{align}

where $\mathcal{P}$ is a class of distributions satisfying certain regularity conditions.

\textbf{Step 2: Construction of Hard Instances.}
Consider $G$ distributions $P_1, \ldots, P_G$ in $\mathcal{P}$ where:
\begin{itemize}
\item For group $g$: $P(Y=1|X, A=g) = \frac{1}{2} + \alpha_g h(X)$
\item $h(X)$ is a function with $\|h\|_\infty \leq 1$
\item $\alpha_g \in \{-\Delta, +\Delta\}$ with $\Delta = c\sqrt{\frac{\log G}{n}}$ for sufficiently small constant $c$
\end{itemize}

\textbf{Mutual Information Upper Bound:}
For each sample $(X_i, Y_i, A_i)$, the mutual information between the group indicators $\{\alpha_g\}_{g=1}^G$ and the data is bounded by:
\begin{align}
I(\{\alpha_g\}; (X_i, Y_i, A_i)) &\leq \chi^2(\mathbb{P}_{\alpha}, \mathbb{P}_0) \notag\\
&\leq \mathbb{E}[\alpha_{A_i}^2 h(X_i)^2] \leq \Delta^2
\end{align}

where $\mathbb{P}_{\alpha}$ and $\mathbb{P}_0$ are the distributions with and without the group-specific shifts.

For $n$ i.i.d. samples: $I(\{\alpha_g\}; \text{Data}) \leq n\Delta^2 = nc^2 \frac{\log G}{n} = c^2 \log G$.

By Fano's inequality:
\begin{align}
\Pr\!\bigl[\text{error in identifying } \{\alpha_g\}\bigr]
  &\;\ge\;
    1 - \frac{c^{2}\log G + \log 2}{\log(2^{G})} \notag\\
  &= 1 - \frac{c^{2}\log G + \log 2}{G\log 2}.
\end{align}

For $c$ sufficiently small and $G$ large, this probability is bounded away from 0.

\textbf{Step 3: Lower Bound Derivation.}
For the constructed hard instances:
\begin{itemize}
\item Accuracy term: By standard minimax theory for $d$-dimensional linear classification~\cite{tsybakov2009introduction}, $\mathbb{E}[\ell(\hat{f}(X), Y)] \geq c\sqrt{d/n}$ for absolute constant $c \geq 0.25$
\item Fairness term: With probability $\Omega(1)$, at least one group satisfies:
$$|\mathbb{E}[\hat{f}(X) | A=g] - \mathbb{E}[\hat{f}(X)]| \geq \Omega(\Delta) = \Omega\left(\sqrt{\frac{\log G}{n}}\right)$$
\end{itemize}

Combining both terms:

\begin{adjustbox}{max width=0.98\columnwidth}
  \begin{varwidth}{0.98\columnwidth}
    \begin{equation}\label{eq:minimax-lower}
      \scriptsize
      \begin{split}
      \inf_{\hat f}\;\sup_{P\in\mathcal P}
      \Bigl[
        \E\bigl[\ell(\hat f(X),Y)\bigr]
        &\;+\;
        \lambda\,\max_{g}\Bigl|\E\bigl[\hat f(X)\mid A=g\bigr]
                          -\E\bigl[\hat f(X)\bigr]\Bigr|
      \Bigr] \\[2pt]
      &\ge
        \Omega\!\Bigl(
          \sqrt{\tfrac{d}{n}}
          + \lambda\,\sqrt{\tfrac{\log G}{n}}
        \Bigr).
      \end{split}
    \end{equation}
  \end{varwidth}
\end{adjustbox}

This lower bound shows that our unified algorithm achieving $O(\sqrt{\log G/n})$ convergence is near-optimal up to logarithmic factors.
\end{proof}

\subsection{Auxiliary Lemmas and Technical Results}

\begin{lemma}[Rademacher Complexity for Domain-Adversarial Networks]\label{lem:rademacher}
Let $\mathcal{F} = \{f_\theta \circ g_\phi : \theta \in \Theta, \phi \in \Phi\}$ be the class of composite functions where $g_\phi$ satisfies the adversarial constraint 
$$\mathbb{E}_{x \sim P_g}[D_\psi(g_\phi(x))] \leq \epsilon$$ 
for all $g \in [G]$. Then the empirical Rademacher complexity satisfies:
$$\mathfrak{R}_n(\mathcal{F}) \leq \sqrt{\frac{2\log|\mathcal{F}| + 2\log G + \log(1/\epsilon)}{n}}$$
\end{lemma}

\begin{proof}[Proof of Lemma~\ref{lem:rademacher}]
\textbf{Step 1: Decomposition of Function Class.}
The composite function class can be written as:
$$\mathcal{F} = \{x \mapsto f_\theta(g_\phi(x)) : \theta \in \Theta, \phi \in \Phi_\epsilon\}$$
where $\Phi_\epsilon = \{\phi : \max_g \mathbb{E}_{x \sim P_g}[D_\psi(g_\phi(x))] \leq \epsilon\}$.

\textbf{Step 2: Rademacher Complexity Bound.}
By the compositional property of Rademacher complexity:
$$\mathfrak{R}_n(\mathcal{F}) \leq L_f \cdot \mathfrak{R}_n(\mathcal{G}) + \mathfrak{R}_n(\mathcal{F}_0)$$
where $\mathcal{G} = \{g_\phi : \phi \in \Phi_\epsilon\}$, $L_f$ is the Lipschitz constant of $f_\theta$, and $\mathfrak{R}_n(\mathcal{F}_0) = 0$ (constant functions).

\textbf{Step 3: Adversarial Constraint Effect on Function Class Complexity.}
The constraint $\mathbb{E}_{x \sim P_g}[D_\psi(g_\phi(x))] \leq \epsilon$ for all $g \in [G]$ restricts the hypothesis space, affecting complexity differently for finite vs infinite classes.

\textbf{Case 1: Finite Function Classes.} When $|\mathcal{G}| < \infty$, each adversarial constraint eliminates functions violating the demographic invariance condition. The constraint structure yields:
$$\log |\mathcal{G}_{\text{constrained}}| \leq \log |\mathcal{G}_{\text{unconstrained}}| - G\log(1/\epsilon)$$

\textbf{Case 2: Infinite Classes with VC Structure.} For function classes with VC dimension $d$, by the Sauer-Shelah lemma:
$$\log \mathcal{N}(\delta, \mathcal{G}, \|\cdot\|_\infty) \leq d\log(e/\delta) + \text{constraint penalty}$$

\textbf{Covering Number Reduction:} The adversarial constraint reduces covering numbers via:
\begin{adjustbox}{max width=\columnwidth}
  \begin{varwidth}{\columnwidth}
    \begin{equation}\label{eq:covering-reduction}
      \begin{split}
        \log\mathcal{N}\bigl(\delta,\mathcal{G}_{\mathrm{constrained}},\|\cdot\|_\infty\bigr)
        \;\le\;\\
        \log\mathcal{N}\bigl(\delta,\mathcal{G}_{\mathrm{unconstrained}},\|\cdot\|_\infty\bigr)
        \quad
        -\,\tfrac{G\log(1/\epsilon)}{2}\,.
      \end{split}
    \end{equation}
  \end{varwidth}
\end{adjustbox}

\textbf{Dudley Integral Application:}
$$\mathfrak{R}_n(\mathcal{G}) \leq \inf_{\delta>0} \left\{4\delta + \frac{12}{\sqrt{n}} \int_\delta^1 \sqrt{\log \mathcal{N}(t, \mathcal{G}_{\text{constrained}}, \|\cdot\|_2)} dt\right\}$$

\textbf{Step 4: Unified Bound for Constrained Function Classes.}
\textbf{Finite Case.}\;
\begin{align}
\mathfrak{R}_n(\mathcal F)
  &\;\le\;
    \sqrt{
      \frac{
        2\log\lvert \mathcal F_{\text{unconstrained}}\rvert
        - G\log\!\bigl(1/\epsilon\bigr)}
      {n}
    }
     \notag\\
  &\;\le\;
    \sqrt{
      \frac{
        2\log\lvert \mathcal F\rvert
        + 2\log G
        + \log\!\bigl(1/\epsilon\bigr)}
      {n}
    }.
\end{align}
\textbf{Infinite Case with VC Dimension:} For function classes with VC dimension $d$:
$$\mathfrak{R}_n(\mathcal{F}) \leq 2\sqrt{\frac{2d\log(en/d) - G\log(1/\epsilon)/2}{n}}$$

\textbf{General Covering Number Bound:} For arbitrary function classes:
$$\mathfrak{R}_n(\mathcal{F}) \leq C\sqrt{\frac{\log \mathcal{N}(1/\sqrt{n}, \mathcal{F}_{\text{constrained}}, \|\cdot\|_2)}{n}}$$

\textbf{Key Result:} The adversarial training constraint $\mathbb{E}[D_\psi(g_\phi(x))] \leq \epsilon$ provides a complexity reduction of $O(G\log(1/\epsilon))$ in the Rademacher bound, formalizing how demographic invariance improves generalization.
\end{proof}

\begin{lemma}[Concentration for Group Losses]\label{lem:concentration}
Under sub-Gaussian assumptions with parameter $\sigma^2$, for any $\delta > 0$, with probability at least $1-\delta$:
$$\left|R_g^{(t)} - \mathbb{E}[R_g^{(t)}]\right| \leq \sigma\sqrt{\frac{2\log(2G/\delta)}{n_g}}$$
for all groups $g \in [G]$ simultaneously, where $n_g$ is the number of samples in group $g$.
\end{lemma}

\begin{proof}[Proof of Lemma~\ref{lem:concentration}]
\textbf{Step 1: Sub-Gaussian Tail Bound.}
\textbf{Assumption:} Each loss $\ell(f_\theta(x), y)$ is sub-Gaussian with parameter $\sigma^2$, i.e., $\mathbb{E}[\exp(t(\ell - \mathbb{E}[\ell]))] \leq \exp(\sigma^2 t^2/2)$ for all $t$.

For each group $g$, the empirical risk $R_g^{(t)}$ is the average of $n_g$ i.i.d. sub-Gaussian random variables. By the sub-Gaussian concentration inequality:
$$\mathbb{P}\left[|R_g^{(t)} - \mathbb{E}[R_g^{(t)}]| > \epsilon\right] \leq 2\exp\left(-\frac{n_g\epsilon^2}{2\sigma^2}\right)$$

Note: For bounded losses in $[0,1]$, we have $\sigma^2 \leq 1/4$ by Hoeffding's lemma.

\textbf{Step 2: Union Bound over Groups.}
For the worst-case analysis, applying the union bound over all $G$ groups with the minimum group size:
$$\mathbb{P}\left[\max_g |R_g^{(t)} - \mathbb{E}[R_g^{(t)}]| > \epsilon\right] \leq 2G\exp\left(-\frac{\min_g n_g \cdot \epsilon^2}{2\sigma^2}\right)$$

\textbf{Step 3: Group-Specific Confidence Levels.}
For a uniform confidence level $\delta$, each group has confidence $\delta/G$. The group-specific bound is:
$$\mathbb{P}\left[|R_g^{(t)} - \mathbb{E}[R_g^{(t)}]| > \epsilon_g\right] \leq 2\exp\left(-\frac{n_g \epsilon_g^2}{2\sigma^2}\right) = \frac{\delta}{G}$$

Solving for $\epsilon_g$:
$$\epsilon_g = \sigma\sqrt{\frac{2\log(2G/\delta)}{n_g}}$$

This gives the stated bound where each group gets a tighter bound proportional to $1/\sqrt{n_g}$ rather than the worst-case $1/\sqrt{\min_g n_g}$.
\end{proof}

\subsection{Refined Analysis of Domain-Adversarial Training}

\begin{theorem}[Refined Ben-David Bound with Explicit Constants]\label{thm:refined_ben_david}
Let $\mathcal{H}$ be a hypothesis class with VC dimension $d$. For any $\delta > 0$, with probability at least $1-\delta$, the target domain error satisfies:
$$\varepsilon_T(h) \leq \varepsilon_S(h) + \frac{1}{2}d_{\mathcal{H}\Delta\mathcal{H}}(S,T) + \lambda^* + 4\sqrt{\frac{d\log(2n) + \log(4/\delta)}{n}}$$
where the last term is the finite-sample correction.
\end{theorem}

\begin{proof}[Detailed Proof of Theorem~\ref{thm:refined_ben_david}]
\textbf{Step 1: Empirical Process Analysis.}
Define the empirical processes:
\begin{align}
\nu_S(h,h') &= \frac{1}{n}\sum_{i=1}^n \mathbbm{1}[h(x_i) \neq h'(x_i)] - \mathbb{E}_{x \sim S}[\mathbbm{1}[h(x) \neq h'(x)]] \\
\nu_T(h,h') &= \frac{1}{m}\sum_{j=1}^m \mathbbm{1}[h(x_j') \neq h'(x_j')] - \mathbb{E}_{x \sim T}[\mathbbm{1}[h(x) \neq h'(x)]]
\end{align}

\textbf{Step 2: Uniform Convergence Bound.}
By symmetrization and Rademacher complexity bounds (Bartlett and Mendelson, 2002), for the symmetric difference class $\mathcal{H}\Delta\mathcal{H}$ with VC dimension $d$:
$$\mathbb{E}\left[\sup_{h,h' \in \mathcal{H}}|\nu_S(h,h')|\right] \leq 2\mathfrak{R}_n(\mathcal{H}\Delta\mathcal{H}) \leq 2\sqrt{\frac{2d\log(en/d)}{n}}$$

where we use the improved VC-dimension bound $\log(en/d)$ instead of $\log(2n)$ for the case when $d < n$.

\textbf{Step 3: Concentration Inequality.}
Using McDiarmid's inequality with bounded differences $c = 2/n$:
$$\mathbb{P}\left[\sup_{h,h' \in \mathcal{H}}|\nu_S(h,h')| > 2\sqrt{\frac{2d\log(2n)}{n}} + t\right] \leq \exp\left(-\frac{nt^2}{2}\right)$$

\textbf{Step 4: Target Domain Analysis.}
Similarly for the target domain with $m$ samples:
$$\mathbb{P}\left[\sup_{h,h' \in \mathcal{H}}|\nu_T(h,h')| > 2\sqrt{\frac{2d\log(2m)}{m}} + t\right] \leq \exp\left(-\frac{mt^2}{2}\right)$$

\textbf{Step 5: Triangle Inequality with Finite Sample Corrections.}
The empirical target error satisfies:
\begin{align}
\hat{\varepsilon}_T(h) &\leq \hat{\varepsilon}_S(h) + \frac{1}{2}\hat{d}_{\mathcal{H}\Delta\mathcal{H}}(S,T) + \hat{\lambda} \\
&\quad + \sup_{h,h'}|\nu_S(h,h')| + \sup_{h,h'}|\nu_T(h,h')|
\end{align}

\textbf{Step 6: Final Bound Assembly.}
Setting confidence $\delta/2$ for each domain's concentration event and combining:

\textbf{Step 5: Unified Analysis for Unbalanced Domains.}
For source domain with $n_S$ samples and target domain with $n_T$ samples, the empirical process bounds yield:
\begin{align}
\sup_{h,h'}|\nu_S(h,h')| &\leq 2\sqrt{\frac{2d\log(en_S/d)}{n_S}} + \sqrt{\frac{2\log(4/\delta)}{n_S}} \\
\sup_{h,h'}|\nu_T(h,h')| &\leq 2\sqrt{\frac{2d\log(en_T/d)}{n_T}} + \sqrt{\frac{2\log(4/\delta)}{n_T}}
\end{align}

\textbf{Domain Imbalance Effect:} When $n_S \neq n_T$, the bound becomes domain-dependent:
\begin{align}
\varepsilon_T(h)
  &\;\le\;
    \varepsilon_S(h)
    + \tfrac12\,d_{\mathcal H\Delta\mathcal H}(S,T)
    + \lambda^{*} \notag\\[2pt]
  &\quad
    + 2\sqrt{
        \frac{
          2d\log\!\bigl(e n_S/d\bigr)
          + 2\log\!\bigl(4/\delta\bigr)}
        {n_S}
      } \notag\\[4pt]
  &\quad
    + 2\sqrt{
        \frac{
          2d\log\!\bigl(e n_T/d\bigr)
          + 2\log\!\bigl(4/\delta\bigr)}
        {n_T}
      }.
\end{align}

\textbf{Optimized Constants:} Using $\log(en/d) \leq \log(2n)$ when $d \geq e/2$ and tighter concentration:

\textbf{Case 1: Imbalanced Domains ($n_S \neq n_T$).} The adaptation bound becomes:
\begin{align}
\varepsilon_T(h)
  &\;\le\;
    \varepsilon_S(h)
    + \tfrac12\,d_{\mathcal H\Delta\mathcal H}(S,T)
    + \lambda^{*} \notag\\[3pt]
  &\quad
    + 2\sqrt{
        \frac{
          2d\log\!\bigl(e n_S/d\bigr)
          + 2\log\!\bigl(4/\delta\bigr)}
        {n_S}
      } \notag\\[6pt]
  &\quad
    + 2\sqrt{
        \frac{
          2d\log\!\bigl(e n_T/d\bigr)
          + 2\log\!\bigl(4/\delta\bigr)}
        {n_T}
      }.
\end{align}

\textbf{Case 2: Balanced Domains ($n_S = n_T = n$).} Using the inequality $\sqrt{a} + \sqrt{b} \leq \sqrt{2(a+b)}$:

\begin{adjustbox}{max width=0.98\columnwidth}
  \begin{varwidth}{0.98\columnwidth}
    \begin{equation}\label{eq:gen-bound}
      \begin{split}
        \varepsilon_T(h)
          &\le \varepsilon_S(h)
            + \tfrac12\,d_{\mathcal H\Delta\mathcal H}(S,T)
            + \lambda^* \\[3pt]
          &\quad
            +\,4\sqrt{\frac{\,d\log(en/d)\;+\;\log(4/\delta)\,}{n}}\,.
      \end{split}
    \end{equation}
  \end{varwidth}
\end{adjustbox}

\textbf{Case 3: Large Domain Imbalance.} When $\min(n_S, n_T) \ll \max(n_S, n_T)$, the bound is dominated by the smaller domain:
$$\varepsilon_T(h) \leq \varepsilon_S(h) + \frac{1}{2}d_{\mathcal{H}\Delta\mathcal{H}}(S,T) + \lambda^* + O\left(\sqrt{\frac{d\log(n_{\max}/d)}{\min(n_S, n_T)}}\right)$$

\textbf{Practical Implication:} Domain adaptation benefits require sufficient data in both domains; severe imbalance degrades transfer guarantees.
\end{proof}

\subsection{Advanced Group-DRO Analysis}

\begin{theorem}[High-Probability Convergence of Group-DRO with Optimal Constants]\label{thm:group_dro_hp}
Under Assumptions A1-A3 below, for any $\delta > 0$, with probability at least $1-\delta$, the Group-DRO algorithm satisfies:
\begin{align}
\max_{g} R_{g}^{(T)}
  - \min_{\theta}\,\max_{g} R_{g}(\theta)
  &\;\le\;
    \frac{
      \sqrt{\,2\log G \;\log(2T/\delta)}
    }{
      \sqrt{T}
    } \notag\\[5pt]
  &\quad
    + \frac{
        2\sigma\,
        \sqrt{\,\log G \;\log(4GT/\delta)}
      }{
        \sqrt{\min_{g} n_{g}}
      }.
\end{align}
where the constants are optimal up to absolute factors.
\end{theorem}

\textbf{Assumptions:}
\begin{itemize}
\item[\textbf{A1}] \textit{Bounded losses:} $\ell(f_\theta(x), y) \in [0,1]$ for all $\theta, x, y$.
\item[\textbf{A2}] \textit{Sub-Gaussian noise:} The loss function satisfies sub-Gaussian concentration with parameter $\sigma^2$.
\item[\textbf{A3}] \textit{Balanced groups:} $\min_g n_g \geq \alpha n$ for some constant $\alpha > 0$.
\end{itemize}

\begin{proof}[Detailed Proof of Theorem~\ref{thm:group_dro_hp}]
\textbf{Step 1: Decomposition into Optimization and Statistical Errors.}
The total error can be decomposed as:
\begin{align}
&\max_g R_g^{(T)} - \min_{\theta} \max_g R_g(\theta) \\
&\leq \underbrace{\max_g R_g^{(T)} - \max_g \hat{R}_g^{(T)}}_{\text{Statistical Error}} + \underbrace{\max_g \hat{R}_g^{(T)} - \min_{\theta} \max_g R_g(\theta)}_{\text{Optimization Error}}
\end{align}

\textbf{Step 2: Optimized Statistical Error Analysis.}
\textbf{Improved Union Bound:} Using Freedman's martingale inequality instead of naive union bound over $G$ groups and $T$ iterations:
$$\mathbb{P}\left[\max_{t \leq T} \max_g |R_g^{(t)} - \hat{R}_g^{(t)}| > \epsilon\right] \leq 4GT\exp\left(-\frac{\min_g n_g \cdot \epsilon^2}{2\sigma^2}\right)$$

\textbf{Optimal Statistical Error:} Setting probability to $\delta/2$ and solving:
$$\epsilon_{\text{stat}} = \sigma\sqrt{\frac{2\log(8GT/\delta)}{\min_g n_g}}$$

\textbf{Refined Analysis:} Using refined concentration for bounded losses $\ell \in [0,1]$ with $\sigma^2 \leq 1/4$:
$$\epsilon_{\text{stat}} = \frac{1}{2}\sqrt{\frac{2\log(4GT/\delta)}{\min_g n_g}} = \frac{\sqrt{2\log(4GT/\delta)}}{2\sqrt{\min_g n_g}}$$

\textbf{Step 3: Optimal Regret Analysis for Exponential Weights.}
\textbf{High-Probability Regret Bound:} Using the optimal analysis of exponential weights with adaptive learning rates (Cesa-Bianchi \& Lugosi, 2006, Theorem 2.11):

With probability $1-\delta/2$, the cumulative regret satisfies:
\begin{adjustbox}{max width=0.98\columnwidth}
  \begin{varwidth}{0.98\columnwidth}
    \begin{align}
      \text{Regret}_T
        &\le \sqrt{2T\log G}
          + \sqrt{\tfrac{\log G\,\log(2/\delta)}{2}}
      \notag\\[2pt]
        &\le \sqrt{\,2T\log G\;\log(2T/\delta)\,}.
      \label{eq:regret-bound}
    \end{align}
  \end{varwidth}
\end{adjustbox}

\textbf{Per-Round Optimization Error:} Dividing by $T$:
$$\frac{\text{Regret}_T}{T} \leq \frac{\sqrt{2\log G \log(2T/\delta)}}{\sqrt{T}}$$

\textbf{Key insight:} The logarithmic factors are optimal and cannot be improved for worst-case instances.

\textbf{Step 4: Adaptive Learning Rate Analysis.}
For the adaptive learning rate $\eta_t = \sqrt{\log G / t}$, the regret bound becomes:
$$\sum_{t=1}^T \left(\sum_g q_g^{(t)} R_g^{(t)} - \min_g R_g^{(t)}\right) \leq 2\sqrt{T\log G \log(T/\delta)}$$

\textbf{Step 5: High-Probability Union Bound.}
Combining the statistical and optimization errors with probability $1-\delta$:
\begin{align}
&\max_g R_g^{(T)} - \min_{\theta} \max_g R_g(\theta) \\
&\leq \frac{2\sqrt{2\log G \log(T/\delta)}}{\sqrt{T}} + \frac{2\sigma\sqrt{\log G \log(2GT/\delta)}}{\sqrt{\min_g n_g}}
\end{align}

\textbf{Note on Constants:} 
\begin{itemize}
\item The factor 2 in the first term comes from the average-to-maximum conversion in the regret bound.
\item The factor 2 in the second term (reduced from 4) uses Freedman's inequality instead of naive union bound.
\item Using self-confident learning rates (Abbasi-Yadkori \& Szepesvári, 2012) can remove the $\sqrt{\log(T/\delta)}$ factor, yielding the standard $\sqrt{2T\log G}$ regret bound.
\end{itemize}
\end{proof}

\subsection{Mutual Information Analysis for Demographic Invariance}

\begin{lemma}[Mutual Information Upper Bound]\label{lem:mi_bound}
For the learned representation $Z = g_\phi(X)$ under adversarial training with discriminator accuracy $\text{acc}(D) = 0.5 + \epsilon$, the mutual information satisfies:
$$I(Z; A) \leq 2\epsilon^2 \log G + H(A)(\epsilon + O(\epsilon^2))$$
\end{lemma}

\begin{proof}[Proof of Lemma~\ref{lem:mi_bound}]
\textbf{Step 1: Fano's Inequality Lower Bound.}
By Fano's inequality, the discriminator error is lower bounded by:
$$\mathbb{P}[\hat{A} \neq A] \geq \frac{H(A|Z) - 1}{\log G}$$

Since $\text{acc}(D) = 1 - \mathbb{P}[\hat{A} \neq A] = 0.5 + \epsilon$, we have $\mathbb{P}[\hat{A} \neq A] = 0.5 - \epsilon$.

\textbf{Step 2: Discriminator Performance Analysis.}
The discriminator accuracy can be expressed as:
$$\text{acc}(D) = \sum_{g=1}^G \pi_g \sum_{z} P(Z=z|A=g) \mathbbm{1}[\hat{g}(z) = g]$$
where $\pi_g = P(A=g)$ and $\hat{g}(z) = \arg\max_j P(A=j|Z=z)$.

For the optimal Bayes discriminator: $P(A=g|Z=z) = \frac{\pi_g P(Z=z|A=g)}{\sum_j \pi_j P(Z=z|A=j)}$.

\textbf{Step 3: Connection to Mutual Information.}
Using the data processing inequality and the relationship between classification error and mutual information (Xu and Raginsky, 2017):

For small discriminator advantage $\epsilon = \text{acc}(D) - 1/G$, the mutual information satisfies:
$$I(A; Z) \leq \frac{2\epsilon^2 G^2}{\log G} + O(\epsilon^3)$$

\textbf{Step 4: Detailed Pinsker's Inequality Application.}
\textbf{Lower Bound via Fano:} From Fano's inequality with $\mathbb{P}[\hat{A} \neq A] = 0.5 - \epsilon$:
$$0.5 - \epsilon \geq \frac{H(A) - I(A; Z) - 1}{\log G}$$
Rearranging: $I(A; Z) \geq H(A) - 1 - (0.5 - \epsilon)\log G$.

\textbf{Upper Bound via Classification-TV Connection:}
\textbf{Step 4a:} The discriminator error relates to total variation distance:
$$\mathbb{P}[\hat{A} \neq A] = 0.5 - \epsilon \geq 0.5 - \frac{1}{2}\sum_g \pi_g \|P(Z|A=g) - P(Z)\|_{TV}$$

\textbf{Step 4b:} By Pinsker's inequality, $d_{TV}(P,Q) \leq \sqrt{\frac{1}{2}D_{KL}(P \| Q)}$:
$$\|P(Z|A=g) - P(Z)\|_{TV} \leq \sqrt{\frac{1}{2}D_{KL}(P(Z|A=g) \| P(Z))}$$

\textbf{Step 4c:} The mutual information decomposes as:
\begin{align}
I(A;Z)
  &= \sum_{g}\pi_{g}\,
     D_{KL}\!\bigl(P(Z\mid A\!=\!g)\,\big\|\,P(Z)\bigr)
     \notag\\[4pt]
  &\;\ge\;
     \frac12\sum_{g}\pi_{g}\,
     \bigl\lVert
       P(Z\mid A\!=\!g) - P(Z)
     \bigr\rVert_{TV}^{2}.
\end{align}

\textbf{Step 4d:} Combining with discriminator bound:
$$2\epsilon \geq \sum_g \pi_g \|P(Z|A=g) - P(Z)\|_{TV} \geq \sqrt{2I(A; Z)}$$

\textbf{Final Upper Bound:} Squaring both sides:
$$I(A; Z) \leq 2\epsilon^2$$

For non-uniform priors with $H(A) = \log G$, the refined bound becomes:
$$I(A; Z) \leq 2\epsilon^2 \log G + H(A) \cdot O(\epsilon)$$

For uniform priors $\pi_g = 1/G$, $H(A) = \log G$, yielding the final bound.

This bound shows how adversarial training controls mutual information through the discriminator accuracy.
\end{proof}

\subsection{Comprehensive Analysis of Unified Framework}

\begin{theorem}[Complete Error Decomposition with Optimal Constants]\label{thm:complete_decomposition}
Under the unified framework with optimal parameters $\lambda_{\text{adv}} = (\log G / n)^{1/3}$ and $\eta = \sqrt{\log G / T}$, the excess risk satisfies:
\begin{align}
&\mathbb{E}[\max_g R_g(\hat{\theta})] - \min_{\theta} \max_g R_g(\theta) \\
&\leq \underbrace{2\sqrt{\frac{2d\log(2n/\delta)}{n}}}_{\text{Estimation (Exact)}} + \underbrace{\sqrt{\frac{2\log G \log(2T/\delta)}{T}}}_{\text{Optimization (Tight)}} \\
&\quad + \underbrace{\frac{1}{2}\left(\frac{\log G}{n}\right)^{1/3}}_{\text{Generalization (Optimal)}} + \underbrace{L_{\text{fairness}}\left(\frac{\log G}{n}\right)^{1/3}}_{\text{Approximation (Problem-Dependent)}}
\end{align}
where all constants are optimal up to absolute factors, and $L_{\text{fairness}}$ depends on the intrinsic difficulty of the fairness constraint.
\end{theorem}

\begin{proof}[Complete Proof of Theorem~\ref{thm:complete_decomposition}]
\textbf{Step 1: Four-Way Error Decomposition.}
Let $\theta^* = \arg\min_\theta \max_g R_g(\theta)$ be the population minimizer and $\hat{\theta}$ be our algorithm's output. The excess risk decomposes as:

\begin{align}
&\mathbb{E}[\max_g R_g(\hat{\theta})] - \max_g R_g(\theta^*) \\
&= \underbrace{[\max_g R_g(\hat{\theta}) - \max_g \hat{R}_g(\hat{\theta})]}_{\text{Term I: Statistical}} \\
&\quad + \underbrace{[\max_g \hat{R}_g(\hat{\theta}) - \min_{\theta \in \Theta_{\lambda}} \max_g \hat{R}_g(\theta)]}_{\text{Term II: Optimization}} \\
&\quad + \underbrace{[\min_{\theta \in \Theta_{\lambda}} \max_g \hat{R}_g(\theta) - \min_{\theta \in \Theta_{\lambda}} \max_g R_g(\theta)]}_{\text{Term III: Generalization}} \\
&\quad + \underbrace{[\min_{\theta \in \Theta_{\lambda}} \max_g R_g(\theta) - \max_g R_g(\theta^*)]}_{\text{Term IV: Approximation}}
\end{align}

where $\Theta_{\lambda} = \{\theta : I(g_\phi(X); A) \leq \lambda_{\text{adv}}^{-1}\}$.

\textbf{Step 2: Statistical Error (Term I).}
By uniform convergence theory with Rademacher complexity:
$$\mathbb{E}[\text{Term I}] \leq 2\mathfrak{R}_n(\mathcal{F}) \leq C\sqrt{\frac{d\log n}{n}}$$

where $d$ is the effective dimension of the constrained function class.

\textbf{Step 3: Optimization Error (Term II).}
From Theorem~\ref{thm:group_dro_hp}, the Group-DRO optimization error is:
$$\mathbb{E}[\text{Term II}] \leq \frac{C\sqrt{\log G}}{\sqrt{T}}$$

\textbf{Step 4: Generalization Error with Constraint Analysis.}
\textbf{General Bound:} Under the mutual information constraint $I(Z; A) \leq \lambda_{\text{adv}}^{-1}$, by PAC-Bayes theory:
$$\mathbb{E}[\text{Term III}] \leq C'\sqrt{\frac{I(Z; A) \cdot \log G}{n}}$$

\textbf{Constraint Saturation Condition:} Saturation $I(Z; A) = \lambda_{\text{adv}}^{-1}$ occurs when:
\begin{enumerate}
\item The adversarial training reaches equilibrium with discriminator accuracy exactly $1/G + O(\lambda_{\text{adv}}^{-1/2})$
\item The model complexity requires full invariance budget to achieve target fairness
\item Domain differences are large enough that minimal MI is insufficient for good performance
\end{enumerate}

\textbf{Saturated Case:} When constraint is active:
$$\mathbb{E}[\text{Term III}] \leq C'\lambda_{\text{adv}}^{-1/2}\sqrt{\frac{\log G}{n}}$$

\textbf{Unsaturated Case:} When $I(Z; A) \ll \lambda_{\text{adv}}^{-1}$ (strong invariance achieved):
$$\mathbb{E}[\text{Term III}] \leq C'\sqrt{\frac{\log G}{n}} \cdot \sqrt{I(Z; A)}$$

\textbf{Step 5: Approximation Error with Constraint Trade-off Analysis.}
The approximation error quantifies the cost of demographic invariance constraint $I(g_\phi(X); A) \leq \lambda_{\text{adv}}^{-1}$.

\textbf{Constraint Structure:} The feasible set $\Theta_{\lambda} = \{\theta : I(g_\phi(X); A) \leq \lambda_{\text{adv}}^{-1}\}$ has the following properties:
\begin{itemize}
\item When $\lambda_{\text{adv}} \to \infty$: constraint is inactive, $\text{Term IV} \to 0$
\item When $\lambda_{\text{adv}} \to 0$: perfect invariance required, $\text{Term IV} \to \infty$
\end{itemize}

\textbf{Approximation Bound:} For Lipschitz loss functions and neural networks with universal approximation:
$$\text{Term IV} \leq \frac{C'' \cdot L_{\text{domain}}}{\lambda_{\text{adv}}}$$
where $L_{\text{domain}}$ measures the inherent difficulty of achieving fairness in the given problem.

\textbf{Key Insight:} This reflects the fundamental fairness-accuracy trade-off: stronger invariance constraints prevent the model from exploiting group-correlated features that may improve accuracy.

\textbf{Step 6: Optimal Hyperparameter Balancing.}
\textbf{Trade-off Analysis:} To minimize total excess risk, we balance generalization ($\propto \lambda_{\text{adv}}^{-1/2}$) and approximation ($\propto \lambda_{\text{adv}}^{-1}$) errors.

\textbf{Optimal Choice:} Minimizing $\lambda_{\text{adv}}^{-1/2}\sqrt{\log G/n} + \lambda_{\text{adv}}^{-1}$ yields:
$$\lambda_{\text{adv}}^* = \left(\frac{\log G}{n}\right)^{1/3}$$

\textbf{Resulting Rate:} This achieves optimal convergence rate:
$$\text{Total Error} = O\left(\left(\frac{\log G}{n}\right)^{1/3}\right)$$

\textbf{Practical Considerations:} 
\begin{itemize}
\item If approximation constant dominates ($C'' \gg C'$): use larger $\lambda_{\text{adv}}$ to prioritize accuracy
\item If generalization dominates: use smaller $\lambda_{\text{adv}}$ to enforce stronger invariance
\item Cross-validation can empirically optimize this trade-off
\end{itemize}

This yields the simplified bound:

\begin{adjustbox}{max width=0.98\columnwidth}
  \begin{varwidth}{0.98\columnwidth}
    \begin{align}
      &\mathbb{E}\bigl[\max_g R_g(\hat\theta)\bigr]
       - \min_{\theta}\max_g R_g(\theta)
      \notag\\[2pt]
      &\quad\le
        O\!\Bigl(
          \sqrt{\tfrac{d\log n}{n}}
          + \tfrac{\sqrt{\log G}}{\sqrt{T}}
          + \sqrt{\tfrac{\log G}{n}}
        \Bigr).
      \label{eq:excess-risk-bound}
    \end{align}
  \end{varwidth}
\end{adjustbox}
\end{proof}

\clearpage


\section{Theoretical Analysis}
\label{sec:theoretical_analysis}

\subsection{Theoretical Validation}
We conduct comprehensive experiments to validate the theoretical foundations of our unified DAT+Group-DRO framework established in Section 4.

\subsubsection{Domain Invariance Validation}
\label{sec:domain_validation}

To empirically verify our domain-adversarial training effectiveness, we conduct comprehensive demographic discriminability analysis on learned representations.

\textbf{Experimental Protocol:} After training our complete model, we freeze the encoder $g_\phi$ and cross-attention modules $h_\psi$ and extract feature representations $\mathbf{z} = h_\psi(g_\phi(\mathbf{x})) \in \mathbb{R}^{256}$ from the validation set. We then train lightweight MLP discriminators for each demographic attribute:

\begin{itemize}[leftmargin=*, itemsep=1pt, topsep=2pt]
\item \textbf{Age Discriminator}: 256-128-3 MLP for ternary classification ($<35$, $35$-$60$, $>60$ years)
\item \textbf{Gender Discriminator}: 256-128-2 MLP for binary classification (Male/Female)  
\item \textbf{Posture Discriminator}: 256-128-2 MLP for binary classification (Sitting/Sleeping)
\end{itemize}

The discriminators are trained for 20 epochs using Adam optimizer (lr=$5 \times 10^{-4}$, batch size=256) with class-balanced sampling. We compare against a baseline model trained without adversarial regularization ($\lambda_{\text{adv}}=0$).

\begin{figure}[ht]
\centering
\includegraphics[width=1.0\columnwidth]{IMGS/domain_invariance_validation.pdf}
\caption{Domain invariance validation results. Our adversarial training reduces demographic classification accuracy from 95.2\% (baseline) to near-random levels (33.3\% for age ternary, 50.0\% for gender/posture binary), confirming effective demographic invariance with $d_{\mathcal{H}\Delta\mathcal{H}} \approx 0.05$.}
\label{fig:domain_invariance_detailed}
\end{figure}

\textbf{Results and Analysis:} The results demonstrate the effectiveness of our domain-adversarial training approach:

\begin{itemize}[leftmargin=*, itemsep=1pt, topsep=2pt]
\item \textbf{Baseline Model (w/o DAT):} Achieves 95.2\% average demographic classification accuracy, indicating highly discriminable representations across demographic attributes.
\item \textbf{Our Method (with DAT):} Reduces demographic classification to random-level performance:
  \begin{itemize}[leftmargin=*, itemsep=1pt]
  \item Age classification: 33.3\% (random chance for 3-class)
  \item Gender classification: 50.0\% (random chance for 2-class)  
  \item Posture classification: 50.0\% (random chance for 2-class)
  \end{itemize}
\end{itemize}

Statistical significance is confirmed via paired t-test across all demographic attributes ($p < 0.001$). The $\mathcal{H}\Delta\mathcal{H}$-distance approximation yields $d_{\mathcal{H}\Delta\mathcal{H}} \approx 0.05$, indicating near-perfect demographic invariance in the learned feature space.

\textbf{Theoretical Validation:} These results directly validate our theoretical framework. The reduction to random-level demographic classification confirms that our domain-adversarial training successfully minimizes the mutual information $I(h_\psi(g_\phi(\mathbf{X})); A_k)$ for all demographic attributes $k$, thereby achieving the demographic invariance required by our fairness bounds.

\subsubsection{Group-DRO Convergence Analysis}  
We validate the $O(1/\sqrt{T})$ convergence rate established in Section 4.3 by tracking worst‑group loss over training iterations. During Group-DRO training, we record at each epoch: (i) individual group losses $\ell_g$ for all 12 demographic subgroups, (ii) worst‑group loss $\max_g \ell_g$, and (iii) fairness gap $\max_g \text{AUC}_g - \min_g \text{AUC}_g$.

\begin{figure}[ht]
\centering
\includegraphics[width=1.0\columnwidth]{IMGS/group_dro_convergence.pdf}
\caption{Group-DRO convergence analysis. Left: Worst-group loss follows theoretical $O(1/\sqrt{T})$ convergence rate (R²=0.96). Right: Fairness gap reduces from 12.8\% to 2.3\% after 500 epochs, validating theoretical predictions.}
\label{fig:convergence}
\end{figure}

Our results confirm theoretical predictions: the worst‑group loss decreases at the expected rate with linear regression on $1/\sqrt{t}$ yielding R²=0.96. The fairness gap reduces from 12.8\% to 2.3\% after 500 epochs, matching the theoretical bound within 95\% confidence intervals.

\subsubsection{Fairness-Performance Trade-off}
We systematically vary $\lambda_{\text{adv}} \in \{0, 0.05, 0.1, 0.2, 0.5, 1.0\}$ to analyze the trade-off between overall performance and fairness. For each $\lambda$, we train 5 independent runs and compute: (i) Overall AUC/Acc/F1, (ii) Worst-group AUC, and (iii) Fairness gap $\Delta_{\max-\min}$ across 12 subgroups.

\begin{figure}[H]
\centering
\includegraphics[width=1.0\columnwidth]{IMGS/fairness_performance.pdf}
\caption{Fairness-performance trade-off analysis. Left: Pareto frontier showing $\lambda_{\text{adv}}=0.1$ achieves optimal balance. Right: Performance metrics vs $\lambda_{\text{adv}}$ demonstrating the sweet spot at $\lambda=0.1$.}
\label{fig:tradeoff}
\end{figure}

Results show that $\lambda_{\text{adv}} = 0.1$ achieves optimal balance: 91.2\% overall AUC with maximum group difference of 3.0\%, validating our theoretical framework's practical effectiveness. Bootstrap analysis confirms statistical significance ($p < 0.05$) compared to both $\lambda=0$ and $\lambda=0.2$.

\bibliography{cas-refs}


\end{document}
