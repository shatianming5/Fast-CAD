<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <meta name="title" content="FAST-CAD:  Fairness-Aware Framework for Non-Contact Stroke Diagnosis">
  <meta name="description" content="FAST-CAD unifies domain-adversarial training with Group-DRO and multimodal self-supervision to deliver 91.2% AUC, a 3.0% fairness gap, and only a 7.5% external drop for non-contact stroke diagnosis across 12 demographic subgroups.">
  <meta name="keywords" content="FAST-CAD, stroke diagnosis, fairness-aware learning, domain adversarial training, Group-DRO, multimodal healthcare AI, self-supervised learning">
  <meta name="author" content="Tianming (Tommy) Sha, Zechuan Chen, Zhan Cheng, Haotian Zhai, Xuwei Ding, Keze Wang">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="FAST-CAD Research">
  <meta property="og:title" content="FAST-CAD: Fairness-Aware Framework for Non-Contact Stroke Diagnosis">
  <meta property="og:description" content="Unified DAT + Group-DRO delivers state-of-the-art, fairness-aware non-contact stroke screening with 91.2% AUC across 12 demographic groups.">
  <meta property="og:url" content="https://shatianming.github.io/fast-cad">
  <meta property="og:image" content="https://shatianming.github.io/fast-cad/static/images/fast_cad_framework.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="FAST-CAD architecture overview">
  <meta property="article:published_time" content="2026-01-15T00:00:00.000Z">
  <!-- Keep only these authors across OG article:author -->
  <meta property="article:author" content="Tianming (Tommy) Sha">
  <meta property="article:author" content="Zechuan Chen">
  <meta property="article:author" content="Zhan Cheng">
  <meta property="article:author" content="Haotian Zhai">
  <meta property="article:author" content="Xuwei Ding">
  <meta property="article:author" content="Keze Wang">
  <meta property="article:section" content="Healthcare AI">
  <meta property="article:tag" content="fairness">
  <meta property="article:tag" content="stroke diagnosis">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@AIResearchLab">
  <meta name="twitter:creator" content="@tommy_sha">
  <meta name="twitter:title" content="FAST-CAD: Fairness-Aware Framework for Non-Contact Stroke Diagnosis">
  <meta name="twitter:description" content="Domain-adversarial training + Group-DRO yields 91.2% AUC while keeping subgroup gaps under 3%.">
  <meta name="twitter:image" content="https://shatianming.github.io/fast-cad/static/images/fast_cad_framework.png">
  <meta name="twitter:image:alt" content="FAST-CAD workflow diagram">

  <!-- Academic/Research Specific (Highwire/Google Scholar) -->
  <meta name="citation_title" content="FAST-CAD: A Fairness-Aware Framework for Non-Contact Stroke Diagnosis">
  <meta name="citation_author" content="Sha, Tianming (Tommy)">
  <meta name="citation_author" content="Chen, Zechuan">
  <meta name="citation_author" content="Cheng, Zhan">
  <meta name="citation_author" content="Zhai, Haotian">
  <meta name="citation_author" content="Ding, Xuwei">
  <meta name="citation_author" content="Wang, Keze">
  <meta name="citation_publication_date" content="2026">
  <meta name="citation_conference_title" content="AAAI Conference on Artificial Intelligence (AAAI)">
  <meta name="citation_pdf_url" content="https://shatianming.github.io/fast-cad/static/pdfs/fast_cad_main.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">

  <title>FAST-CAD: Fairness-Aware Stroke Diagnosis</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "FAST-CAD: A Fairness-Aware Framework for Non-Contact Stroke Diagnosis",
    "description": "FAST-CAD integrates domain-adversarial training with Group-DRO and multimodal self-supervision to deliver 91.2% AUC with a 3.0% subgroup gap for non-contact stroke diagnosis.",
    "author": [
      {
        "@type": "Person",
        "name": "Tianming (Tommy) Sha",
        "affiliation": {
          "@type": "Organization",
          "name": "Stony Brook University"
        }
      },
      {
        "@type": "Person",
        "name": "Zechuan Chen",
        "affiliation": {
          "@type": "Organization",
          "name": "University of Wisconsin-Madison"
        }
      },
      {
        "@type": "Person",
        "name": "Zhan Cheng",
        "affiliation": {
          "@type": "Organization",
          "name": "University of Wisconsin-Madison"
        }
      },
      {
        "@type": "Person",
        "name": "Haotian Zhai",
        "affiliation": {
          "@type": "Organization",
          "name": "Sun Yat-sen University"
        }
      },
      {
        "@type": "Person",
        "name": "Xuwei Ding",
        "affiliation": {
          "@type": "Organization",
          "name": "University of Wisconsin-Madison"
        }
      },
      {
        "@type": "Person",
        "name": "Keze Wang",
        "affiliation": {
          "@type": "Organization",
          "name": "Sun Yat-sen University"
        }
      }
    ],
    "datePublished": "2026-01-15",
    "publisher": {
      "@type": "Organization",
      "name": "AAAI Conference on Artificial Intelligence"
    },
    "url": "https://shatianming.github.io/fast-cad",
    "image": "https://shatianming.github.io/fast-cad/static/images/fast_cad_framework.png",
    "keywords": [
      "stroke diagnosis",
      "fairness",
      "domain adversarial training",
      "Group-DRO",
      "multimodal learning",
      "medical AI"
    ],
    "abstract": "Stroke is an acute cerebrovascular disease, and timely diagnosis significantly improves patient survival. However, existing automated diagnosis methods suffer from fairness issues across demographic groups, potentially exacerbating healthcare disparities. We propose FAST-CAD, a theoretically grounded framework that integrates Domain-Adversarial Training with Group Distributionally Robust Optimization for fair and accurate non-contact stroke diagnosis. Our multimodal dataset spans 12 demographic subgroups defined by age, gender, and posture. Self-supervised encoders paired with adversarial domain discriminators learn demographic-invariant representations, and Group-DRO optimizes worst-group risk. FAST-CAD attains 91.2% AUC with subgroup gaps under 3% while theoretical analysis confirms the effectiveness of the unified DAT + Group-DRO objective, offering practical advances and insights for fair medical AI.",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https://shatianming.github.io/fast-cad"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "Stroke diagnosis"
      },
      {
        "@type": "Thing",
        "name": "Fairness in medical AI"
      },
      {
        "@type": "Thing",
        "name": "Multimodal representation learning"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "AI for Equitable Healthcare Lab",
    "url": "https://shatianming.github.io",
    "logo": "https://shatianming.github.io/fast-cad/static/images/favicon.ico",
    "sameAs": [
      "https://github.com/shatianming",
      "https://twitter.com/AIResearchLab"
    ]
  }
  </script>
</head>
<body>

  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <!-- More Works Dropdown -->
  <div class="more-works-container">
    <button class="more-works-btn" onclick="toggleMoreWorks()" title="Explore FAST-CAD resources">
      <i class="fas fa-flask"></i>
      FAST-CAD Resources
      <i class="fas fa-chevron-down dropdown-arrow"></i>
    </button>
    <div class="more-works-dropdown" id="moreWorksDropdown">
      <div class="dropdown-header">
        <h4>Explore FAST-CAD Materials</h4>
        <button class="close-btn" onclick="toggleMoreWorks()">
          <i class="fas fa-times"></i>
        </button>
      </div>
      <div class="works-list">
        <a href="#dataset" class="work-item">
          <div class="work-info">
            <h5>Dataset Card</h5>
            <p>Acquisition protocol, subgroup stratification, and ethics notes for the 243-subject cohort.</p>
            <span class="work-venue">Clinical Data Release 2026</span>
          </div>
          <i class="fas fa-arrow-right"></i>
        </a>
        <a href="#methodology" class="work-item">
          <div class="work-info">
            <h5>Theoretical Appendix</h5>
            <p>Unified DAT + Group-DRO derivations with convergence and fairness bounds.</p>
            <span class="work-venue">AAAI Theory Track</span>
          </div>
          <i class="fas fa-arrow-right"></i>
        </a>
        <a href="#results" class="work-item">
          <div class="work-info">
            <h5>Evaluation Suite</h5>
            <p>Benchmark comparisons, fairness diagnostics, and external cohort validation.</p>
            <span class="work-venue">Comprehensive Results</span>
          </div>
          <i class="fas fa-arrow-right"></i>
        </a>
      </div>
    </div>
  </div>

  <main id="main-content">
    <section class="hero publication-header">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="paper-intro">
              <h1 class="title is-1 publication-title">FAST-CAD: A Fairness-Aware Framework for Non-Contact Stroke Diagnosis</h1>
              <p class="hero-tagline">Self-supervised multimodal sensing + unified Domain-Adversarial Training and Group-DRO enable equitable, contact-free stroke screening.</p>
              <div class="is-size-5 publication-authors">
                <span class="author-block"><strong>First Author:</strong> <a href="mailto:tianming.sha@stonybrook.edu">Tianming (Tommy) Sha</a><sup>1</sup></span><br>
                <span class="author-block">Zechuan Chen<sup>2</sup>, Zhan Cheng<sup>2</sup>, Haotian Zhai<sup>3</sup>, Xuwei Ding<sup>2</sup>, <strong>Corresponding:</strong> Keze Wang<sup>4†</sup></span>
              </div>
              <div class="is-size-6 publication-authors">
                <span class="author-block">1 Stony Brook University &middot; 2 University of Wisconsin-Madison &middot; 3 Sun Yat-sen University</span>
                <span class="eql-cntrb"><small><br><sup>†</sup>Corresponding author</small></span>
              </div>
              <p class="publication-venue">AAAI 2026 &middot; <span class="oral-badge">Oral</span></p>
              <div class="publication-links">
                <span class="link-block">
                  <a href="static/pdfs/fast_cad_main.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fas fa-file-pdf"></i></span>
                    <span>Paper PDF</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2511.08887" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fas fa-globe"></i></span>
                    <span>arXiv 2511.08887</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="#dataset" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fas fa-database"></i></span>
                    <span>Dataset Overview</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="#results" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fas fa-chart-line"></i></span>
                    <span>Results &amp; Tables</span>
                  </a>
                </span>
              </div>
            </div>
        </div>
      </div>
    </section>

    <section class="hero teaser" id="overview">
      <div class="container is-max-desktop">
        <div class="hero-body">
          <div class="columns is-vcentered">
            <div class="column">
              <h2 class="title is-3">Why FAST-CAD?</h2>
              <p>FAST-CAD bridges the gap between hospital-bound imaging and low-accuracy manual FAST checks by fusing facial, tongue, limb, and speech cues captured with commodity RGB-D cameras and microphones.</p>
              <ul class="highlight-list">
                <li>Unified optimization couples Domain-Adversarial Training with Group-DRO for provably bounded subgroup gaps.</li>
                <li>Self-supervised SeCo and HuBERT encoders remove the need for large labeled medical corpora.</li>
                <li>Clinically interpretable outputs and fairness diagnostics enable deployment in community screening and telemedicine.</li>
              </ul>
            </div>
            <div class="column">
              <figure class="figure-frame">
                <img src="static/images/fast_cad_workflow.png" alt="Comparison of conventional and FAST-CAD stroke workflows" loading="lazy">
                <figcaption>FAST-CAD delivers actionable triage decisions within the golden window without physical contact or hospital visits.</figcaption>
              </figure>
            </div>
          </div>
          <div class="info-card-grid">
            <div class="info-card">
              <h4>Self-Supervised Encoders</h4>
              <p>Frozen SeCo video and HuBERT audio backbones extract 768-d representations that capture facial asymmetry, tongue motion, limb coordination, and dysarthria.</p>
            </div>
            <div class="info-card">
              <h4>Fairness-First Optimization</h4>
              <p>Adversarial heads drive demographic invariance while Group-DRO maintains Rawlsian guarantees via adaptive group weights.</p>
            </div>
            <div class="info-card">
              <h4>Clinical Readiness</h4>
              <p>Deployment on low-cost tablets, automatic subgroup auditing, and convergence proofs increase practitioner trust.</p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section hero is-light" id="abstract">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>Stroke is an acute cerebrovascular disease, and timely diagnosis significantly improves patient survival. However, existing automated diagnosis methods suffer from fairness issues across demographic groups, potentially exacerbating healthcare disparities. We address this challenge with FAST-CAD, a theoretically grounded framework that integrates Domain-Adversarial Training (DAT) with Group Distributionally Robust Optimization (Group-DRO) for fair and accurate non-contact stroke diagnosis. FAST-CAD leverages domain adaptation theory and minimax fairness to provide convergence guarantees and subgroup risk bounds while learning from a multimodal dataset spanning 12 demographic subgroups defined by age, gender, and posture. Self-supervised encoders paired with adversarial domain discriminators learn demographic-invariant representations, and Group-DRO optimizes worst-group risk. FAST-CAD attains 91.2% AUC with subgroup gaps under 3% while theoretical analysis confirms the effectiveness of the unified DAT + Group-DRO objective, offering practical advances and insights for fair medical AI.</p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="hero is-small" id="gallery">
      <div class="hero-body">
        <div class="container">
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item">
              <img src="static/images/fast_cad_workflow.png" alt="FAST-CAD assisted workflow" loading="lazy"/>
              <h2 class="subtitle has-text-centered">The framework integrates with telemedicine triage to cover the stroke golden window.</h2>
            </div>
            <div class="item">
              <img src="static/images/fast_cad_fairness.png" alt="Fairness comparison chart" loading="lazy"/>
              <h2 class="subtitle has-text-centered">FAST-CAD narrows subgroup gaps and raises worst-group AUCs compared with prior art.</h2>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section" id="dataset">
      <div class="container is-max-desktop">
        <div class="columns is-vcentered">
          <div class="column">
            <div class="section-heading">
              <span class="section-eyebrow">Dataset</span>
              <h2 class="title is-3">Demographically Stratified Dataset</h2>
              <p>We curate 243 subjects across 12 demographic combinations (age × gender × posture) with synchronized RGB video, depth, audio, and keypoint annotations. Collection followed medical ethics protocols under clinician supervision, and the 4:1 split with 5-fold cross-validation is aligned with clinical reporting.</p>
            </div>
            <ul class="highlight-list">
              <li>Balanced representation across age brackets (&lt;35, 35-60, &gt;60) and postures (sitting, sleeping).</li>
              <li>Parallel recording of facial asymmetry, tongue motion, arm drift, and speech for richer FAST cues.</li>
              <li>Automatic demographic auditing enables fairness diagnostics and targeted data acquisition.</li>
            </ul>
            <div class="table-wrapper">
              <h3 class="section-subtitle">Cohort Composition</h3>
              <table class="data-table">
                <thead>
                  <tr>
                    <th>Category</th>
                    <th>Subcategory</th>
                    <th>Count (%)</th>
                  </tr>
                </thead>
                <tbody>
                  <tr><td>Age</td><td>&lt; 35</td><td>65 (26.7)</td></tr>
                  <tr><td>Age</td><td>35-60</td><td>96 (39.5)</td></tr>
                  <tr><td>Age</td><td>&gt; 60</td><td>82 (33.7)</td></tr>
                  <tr><td>Gender</td><td>Male</td><td>145 (59.7)</td></tr>
                  <tr><td>Gender</td><td>Female</td><td>98 (40.3)</td></tr>
                  <tr><td>Posture</td><td>Sitting</td><td>149 (61.3)</td></tr>
                  <tr><td>Posture</td><td>Sleeping</td><td>94 (38.7)</td></tr>
                </tbody>
              </table>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section hero is-light" id="methodology">
      <div class="container is-max-desktop">
        <div class="methodology-copy">
          <div class="section-heading">
            <span class="section-eyebrow">Methodology</span>
            <h2 class="title is-3">Unified DAT + Group-DRO Training</h2>
            <p>FAST-CAD alternates between Group-DRO importance updates and demographic adversaries to minimize worst-group risk while enforcing invariance.</p>
          </div>
          <ol class="method-steps">
            <li><strong>Group reweighting:</strong> update importance weights <code>q_g</code> via exponentiated gradients to track worst-performing subgroups.</li>
            <li><strong>Domain-adversarial objectives:</strong> gradient-reversal heads for age, gender, and posture minimize <code>I(z; a)</code> with empirical <code>d_{H-Delta-H} ~ 0.05</code>.</li>
            <li><strong>Fair classification:</strong> optimize <code>L_total = sum_g q_g L_g^{cls} + lambda_adv L_adv</code> with alternating dual-stream fusion that preserves modality balance.</li>
          </ol>
          <p>Theoretical analysis yields <code>O(sqrt(log G / T))</code> convergence and fairness bounds linking worst-group risk to mutual information penalties.</p>
        </div>
        <figure class="figure-frame methodology-figure full-width-figure">
          <img src="static/images/fast_cad_framework.png" alt="FAST-CAD architecture" loading="lazy">
          <figcaption>Frozen encoders feed projection heads, fairness-aware fusion, and demographic discriminators with gradient reversal.</figcaption>
        </figure>
      </div>
    </section>

    <section class="section" id="fairness">
      <div class="container is-max-desktop">
        <div class="fairness-copy">
          <div class="section-heading">
            <span class="section-eyebrow">Fairness</span>
            <h2 class="title is-3">Fairness Diagnostics</h2>
            <p>Demographic discriminators trained on learned representations drop to random accuracy (age 33.3%, gender/posture 50.0%), validating domain invariance. Group-DRO traces the theoretical O(1/sqrt(T)) slope, while subgroup risk gaps fall below 1.7 percentage points.</p>
          </div>
          <div class="info-card-grid">
            <div class="info-card">
              <h4>Domain Gap</h4>
              <p><code>d_{H-Delta-H}</code> plateaus near <strong>0.05</strong>, indicating demographic discriminators cannot distinguish age/gender/posture partitions after adaptation, which keeps subgroup decision boundaries aligned.</p>
            </div>
            <div class="info-card">
              <h4>Mutual Information</h4>
              <p>Dual-stream fusion with adversarial heads constrains <code>I(z; a)</code> to <strong>&le; 0.02</strong>, so no modality leaks demographic cues—an essential precondition for Rawlsian fairness guarantees.</p>
            </div>
            <div class="info-card">
              <h4>Robust Optimization</h4>
              <p>Exponentiated-gradient Group-DRO spikes minority weights when their loss rises, steering updates toward worst-group AUC and yielding <strong>3.0%</strong> max-min gaps without extra supervision.</p>
            </div>
          </div>
        </div>
        <figure class="figure-frame fairness-figure full-width-figure">
          <img src="static/images/fast_cad_fairness.png" alt="Fairness comparison figure" loading="lazy">
          <figcaption>FAST-CAD improves both average and worst-group AUC while shrinking subgroup dispersion.</figcaption>
        </figure>
      </div>
    </section>

    <section class="section" id="results">
      <div class="container is-max-desktop">
        <div class="section-heading">
          <span class="section-eyebrow">Results</span>
          <h2 class="title is-3">Results &amp; Tables</h2>
          <p>FAST-CAD sets a new operating point for non-contact stroke screening by delivering state-of-the-art accuracy, fairness, and domain robustness.</p>
        </div>
        <div class="table-grid table-grid-two">
          <div class="table-wrapper table-span-2">
            <div class="table-header">
              <h3 class="section-subtitle">Comparison with Prior Work</h3>
              <span class="table-tag">Benchmark Suite</span>
            </div>
            <div class="table-scroll">
            <table class="data-table">
              <thead>
                <tr>
                  <th>Method</th>
                  <th>Input</th>
                  <th>AUC</th>
                  <th>Acc</th>
                  <th>F1</th>
                  <th>Sens</th>
                  <th>Spec</th>
                </tr>
              </thead>
              <tbody>
                <tr><td>I3D</td><td>RGB</td><td>68.1&nbsp;&plusmn;&nbsp;9.7</td><td>70.9&nbsp;&plusmn;&nbsp;10.6</td><td>75.8&nbsp;&plusmn;&nbsp;9.7</td><td>65.2</td><td>73.1</td></tr>
                <tr><td>TimeSformer</td><td>RGB</td><td>74.4&nbsp;&plusmn;&nbsp;7.2</td><td>79.9&nbsp;&plusmn;&nbsp;6.2</td><td>85.4&nbsp;&plusmn;&nbsp;6.3</td><td>72.3</td><td>78.1</td></tr>
                <tr><td>DeepStroke</td><td>Multi</td><td>84.5&nbsp;&plusmn;&nbsp;5.6</td><td>76.2&nbsp;&plusmn;&nbsp;5.9</td><td>82.3&nbsp;&plusmn;&nbsp;4.7</td><td>82.1</td><td>85.1</td></tr>
                <tr><td>VideoMAE</td><td>RGB</td><td>81.0&nbsp;&plusmn;&nbsp;3.2</td><td>78.2&nbsp;&plusmn;&nbsp;5.6</td><td>82.7&nbsp;&plusmn;&nbsp;4.8</td><td>78.4</td><td>82.1</td></tr>
                <tr><td>M3Stroke</td><td>Multi</td><td>86.3&nbsp;&plusmn;&nbsp;4.3</td><td>79.2&nbsp;&plusmn;&nbsp;3.9</td><td>84.2&nbsp;&plusmn;&nbsp;4.2</td><td>84.1</td><td>87.2</td></tr>
                <tr><td>wav2vec 2.0</td><td>Audio</td><td>63.1&nbsp;&plusmn;&nbsp;3.7</td><td>71.6&nbsp;&plusmn;&nbsp;4.7</td><td>73.4&nbsp;&plusmn;&nbsp;6.8</td><td>59.8</td><td>75.3</td></tr>
                <tr><td>WavLM</td><td>Audio</td><td>68.4&nbsp;&plusmn;&nbsp;3.8</td><td>72.8&nbsp;&plusmn;&nbsp;4.3</td><td>74.9&nbsp;&plusmn;&nbsp;5.2</td><td>66.2</td><td>76.4</td></tr>
                <tr><td>Cross-Attention</td><td>Multi</td><td>88.6&nbsp;&plusmn;&nbsp;2.2</td><td>83.1&nbsp;&plusmn;&nbsp;4.8</td><td>87.2&nbsp;&plusmn;&nbsp;3.4</td><td>86.4</td><td>89.1</td></tr>
                <tr class="highlight-row"><td>FAST-CAD</td><td>Multi</td><td>91.2&nbsp;&plusmn;&nbsp;1.5</td><td>87.2&nbsp;&plusmn;&nbsp;3.1</td><td>90.8&nbsp;&plusmn;&nbsp;2.3</td><td>89.1</td><td>92.3</td></tr>
              </tbody>
            </table>
            </div>
          </div>
        </div>
        <div class="table-grid table-grid-three">
          <div class="table-wrapper">
            <div class="table-header">
              <h3 class="section-subtitle">Fairness Metrics</h3>
              <span class="table-tag accent">Equity Gap</span>
            </div>
            <div class="table-scroll">
            <table class="data-table">
              <thead>
                <tr>
                  <th>Method</th>
                  <th>Worst AUC</th>
                  <th>Delta max-min</th>
                  <th>Gini</th>
                </tr>
              </thead>
              <tbody>
                <tr><td>Maximum (MViT)</td><td>75.2%</td><td>8.0%</td><td>0.042</td></tr>
                <tr><td>General Transformer*</td><td>81.8%</td><td>5.8%</td><td>0.026</td></tr>
                <tr class="highlight-row"><td>FAST-CAD</td><td>89.5%</td><td>3.0%</td><td>0.011</td></tr>
              </tbody>
            </table>
            </div>
            <p class="table-note">*Feature-concatenation variant built on the same encoders.</p>
          </div>
          <div class="table-wrapper">
            <div class="table-header">
              <h3 class="section-subtitle">Modality Fusion Study</h3>
              <span class="table-tag">Design Ablation</span>
            </div>
            <div class="table-scroll">
            <table class="data-table">
              <thead>
                <tr>
                  <th>Modalities</th>
                  <th>AUC</th>
                  <th>Delta vs Face</th>
                  <th>Params</th>
                </tr>
              </thead>
              <tbody>
                <tr><td>Face</td><td>82.1&nbsp;&plusmn;&nbsp;5.1</td><td>--</td><td>28M</td></tr>
                <tr><td>Face + Tongue</td><td>85.7&nbsp;&plusmn;&nbsp;4.7</td><td>+3.6</td><td>38M</td></tr>
                <tr><td>Face + Tongue + Body</td><td>88.3&nbsp;&plusmn;&nbsp;2.2</td><td>+6.2</td><td>45M</td></tr>
                <tr class="highlight-row"><td>All Modalities</td><td>91.2&nbsp;&plusmn;&nbsp;1.5</td><td>+9.1</td><td>59M</td></tr>
              </tbody>
            </table>
            </div>
          </div>
          <div class="table-wrapper">
            <div class="table-header">
              <h3 class="section-subtitle">Cross-Domain Validation</h3>
              <span class="table-tag accent">External Cohort</span>
            </div>
            <div class="table-scroll">
            <table class="data-table">
              <thead>
                <tr>
                  <th>Method</th>
                  <th>Original AUC</th>
                  <th>External AUC</th>
                  <th>Drop</th>
                </tr>
              </thead>
              <tbody>
                <tr><td>MViT</td><td>78.0%</td><td>65.3%</td><td>-12.7%</td></tr>
                <tr><td>M3Stroke</td><td>86.3%</td><td>71.6%</td><td>-14.7%</td></tr>
                <tr class="highlight-row"><td>FAST-CAD</td><td>91.2%</td><td>83.7%</td><td>-7.5%</td></tr>
              </tbody>
            </table>
            </div>
            <p class="table-note">External cohort: 86 participants recorded in home and telemedicine settings.</p>
          </div>
        </div>
      </div>
    </section>

    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <div class="bibtex-header">
          <h2 class="title">BibTeX</h2>
          <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
            <i class="fas fa-copy"></i>
            <span class="copy-text">Copy</span>
          </button>
        </div>
        <pre id="bibtex-code"><code>@misc{sha2025fastcadfairnessawareframeworknoncontact,
  title={FAST-CAD: A Fairness-Aware Framework for Non-Contact Stroke Diagnosis},
  author={Tianming (Tommy) Sha and Zechuan Chen and Zhan Cheng and Haotian Zhai and Xuwei Ding and Keze Wang},
  year={2025},
  eprint={2511.08887},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2511.08887}
}</code></pre>
      </div>
    </section>
  </main>

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>
</html>
